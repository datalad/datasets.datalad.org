#!/bin/bash

export PS1='$> '
#export PS4="> "
# Let's also log the time etc
export PS4='\n> # $(date +%T.%N)\n> '

# So that git doesn't prompt for password -- we have no ability to see.  We
# just fail and will need to deal with those (e.g. move aside if no longer
# available or address some other way)
export GIT_ASKPASS=/bin/echo 

set -eu

# A rudimentary update script which would take care about auto-updating on cron
# some selected set of the datasets.  This would help to outline use cases we
# would like to have addressed.

# Use our "centralized" environment.
source /home/yoh/proj/datalad/datalad-crawl-env/bin/activate

datalad wtf --flavor short

# override possibly present local configuration for parallel
# datalad operation if python is too old
python --version | awk '{print $2;}' | grep -q '^3\.[891]' || export DATALAD_RUNTIME_MAX__JOBS=1

# Check externals
hash datalad
hash sem
hash chronic


cd "/mnt/btrfs/datasets/datalad/crawl"

# TODO: might want to just end up using singularity environment which to
# include here!
njobs=12


run1() {
    #echo "Running $@ under $PWD"
    # no chronic for now due to https://github.com/datalad/datalad/issues/5465
    #chronic "$@"
    "$@"
}
export -f run1

runj() {
    #echo "Submitting $@ under $PWD"
    #sem -j$njobs "sleep 2; echo Done running $@"
    sem -j$njobs "$@"
    #echo "Done submitting $2"
}

doall() {
    func="$1"
    shift
    # TODO: parallelize this part
    for d in "$@"; do
       runj "$func" "$d";
    done
    sem --wait
    # And then save each at a time but without parallel since we would be
    # conflicting
    #for d in "$@"; do
    if [ "$#" -lt 5 ]; then
        msg="$@"
    else
        msg="$# paths"
    fi
    run1 datalad save -d . -m "Updates from looking at $msg" "$@"
    #done
}
export -f doall

datalad_update_ff_r() (
    msg="Fast forward $1 recursively"
    annex_get_auto=
    recurse_opts="-r"
    if [ "$#" -ge 2 ]; then
        case "$2" in
        -R1)
            recurse_opts+=" -R 1";;
        --annex-get-auto)
            # due to https://github.com/datalad/datalad/issues/4010
            annex_get_auto=1;;
        *)
            echo "Unknown option $2" >&2;
            exit 1;
        esac
    fi
    #-x echo "$msg"
    # ?TODO -s origin
    # ?TODO -R1((e.g. for dandisets to not go into zarrs)
    run1 datalad update -d "$1" $recurse_opts --follow parentds --merge ff-only
    # get all possibly new subdatasets, update would not do that
    run1 datalad get $recurse_opts -d "$1" -J5 -n
    if [ -n "$annex_get_auto" ]; then
        run1 git -C "$1" annex get --auto $recurse_opts
    fi
    datalad save -d '.' -m "$msg" "$1"
)
export -f datalad_update_ff_r

pull_ff_only() (
    run1 git -C "$1" pull --ff-only
    datalad save -d "." -m "Fast forward $1" "$1"
)
export -f pull_ff_only

datalad_crawl() (
    #-x echo "Crawl $1"
    cd "$1"
    run1 datalad crawl
)
export -f datalad_crawl

datalad_crawl_doall_update() (
    msg="Recrawl and update $1"
    #-x echo "$msg"
    set +x  # could be too many
    (
    cd "$1"
    run1 datalad crawl
    # Here using git module name which could differ from path
    subds=( $(datalad -f '{gitmodule_name}' subdatasets -r -R 1) )
    doall datalad_update_ff_r ${subds[*]}
    )
    set -x
    datalad save -d '.' -m "$msg" "$1"
)
export -f datalad_crawl_doall_update

set -x

#
# Straightforward updates from other git repos
#

if [ $# -ge 1 ]; then
   dss=( $* )
 else
   # ad-hoc way to just go through all "registered"
   dss=( $( seq 1 7 ) )
 fi

for ds in ${dss[@]}; do
   case "$ds" in
     1| dandisets)
       # Here we crawl top but then update children in parallel
       datalad_update_ff_r dandi/dandisets -R1;;

     2| dgenomes)
       datalad_update_ff_r dgenomes;;
     
     3| psychoinformatics-de/hirni-toolbox)
       datalad_update_ff_r psychoinformatics-de/hirni-toolbox --annex-get-auto;;

     5| datalad/datalad-course | datalad-course)
       datalad_update_ff_r datalad/datalad-course;;
     
     6| templateflow)
       pull_ff_only templateflow;;
     
     7| dbic/QA)
       datalad_update_ff_r dbic/QA --annex-get-auto;;

     102| openneuro)
       # Here we crawl top but then update children in parallel
       datalad_crawl_doall_update \
           openneuro;;
     
     103| openneuro-derivatives)
       datalad_update_ff_r openneuro-derivatives -R1;;

     104| conp-dataset) #doall
       datalad_update_ff_r \
           conp-dataset
           # might want only tags  templateflow \
       ;;


     *) echo "unknown dataset $ds"; exit 1;;
  esac
done

datalad diff -d . --from datalad-public/master --to HEAD -r 

# Publish since we got so far! last version to have publish 0.14.8
# datalad publish --since= --to=datalad-public --missing=inherit -r

# datalad >= 0.15.0 should be smth like
# TODO: but here we might decide to not do that for some subdatasets!
#  how to make it possible to provision that?!
datalad create-sibling -s datalad-public -r --since=^ --inherit --existing=skip
datalad push --since=^ --data auto -r --to=datalad-public -J5

echo "done"

datalad wtf -S datalad -S dependencies -S extensions

